# Multi-Threaded Scraper built with Selenium and Beautiful Soup
### By: Pakin Wirojwatanakul, Cornell University (Computer Science and Mathematics)


# Overview
This multi-threaded scraper creates a json dataset of wells with all the corresponding information (e.g. operator, name, location, production, etc) from the California website. It also scrapes the oil and gas production information for each well. 

# Main Libraries Used
* Selenium
* Beutiful Soup
* threading
* pandas

# Files Explanation

## `scraper_threaded.py` 
Main scraper file.

**run**

`scraper_threaded.py start_idx end_idx`

**description**

The script reads the `exported.csv` file and extracts a list of well apis. Then it scrapes the well apis from the extracted well api list starting from `start_idx` (inclusive) and ending in `end_idx` (exclusive). The script utilizes four threads and scrapes the apis concurrently. Each thread scrapes a quater of the total apis that was assigned. The script returns 4 list of json objects with the well and production data ( the data format will be discussed in a bit). The list of jsons are named  `well_info_{start_idx}_{q1}.json`, `well_info_{q1}_{q2}.json`, `well_info_{q2}_{q3}.json`, and `well_info_{q3}_{end_idx}.json`. The four lists are dumped as individual json files in the directory `./well_info_jsons_threaded`.  

**example**

`scraper_threaded.py 0 1000`

This command will scrape 1,000 well apis corresponding to the 0th (inclusive) till 1000th (exclusive) entry of the `exported.csv` file. The script will dump the four list of json objects to `./well_info_jsons_threaded/` with the names `well_info_0_250.json`,`well_info_250_500.json`, `well_info_500_750.json`, and `well_info_750_1000.json`.



## `util.py` 
Contains helper functions for `scraper_threaded.py`.


#### `def create_well_info_by_api(driver, api)`

**Description:**

Uses the `driver` to scrape the well information of the well with api = `api`. 
Returns a json object with the corresponding information (data format discussed below)


#### ` def scrape(thread_name, start_idx, end_idx)`


**Description:**

The script reads the `exported.csv` file and extracts a list of well apis. Then it scrapes the well apis from the extracted well api list starting from `start_idx` (inclusive) and ending in `end_idx` (exclusive). The script returns a list of json objects with the well and production data ( the data format will be discussed in a bit). The list of json is named  `well_info_{start_idx}_{end_idx}.json` and is dumped as a json file in the directory `./well_info_jsons_threaded`.     

**Example:**

`scrape("thread_1", 100, 1000)` 

This command will scrape 900 well apis corresponding to the 100th (inclusive) till 1000th (exclusive) entry of the `exported.csv` file. The script will output the list of json objects to `./well_info_jsons_threaded/well_info_100_1000.json`.




##  `exported.csv` contains the complete list of well apis as of Saturday March 23, 2019.
* Downloaded from the well search site and removed the first two blank rows for parsability. 


## `well_info_jsons_threaded` a directory containing all the scraped data. 
* Each file in this directory is generated by the `scrape_threaded.py` script.  
* Each file has the naming convention `well_info_{start_idx}_{end_idx}.json`
* Ex) `well_info_0_1000.json`




## Data Format

Each `./well_info_jsons_threaded/well_info_{start_idx}_{end_idx}.json` file contains a list of json objects with the structure described below. The entries of the `oil_gas_production` list has the format `(date, Oil(bbl), Gas(Mcf))`.


### Sample Data

```python
{'api_num': '01120374',
 'lease_name': 'Froh',
 'well_num': '13-3',
 'country_name': 'Colusa [011]',
 'district_num': '6',
 'operator_name': 'California Resources Production Corporation [C0885]',
 'field_name': 'Grimes Gas [282]',
 'area_name': 'Any Area [00]',
 'section_num': '13',
 'township_num': '14N',
 'range_num': '01W',
 'base_meridian_num': 'MD',
 'well_status': 'Active',
 'well_type': 'DG',
 'spud_date': '1/27/1986',
 'gis_source': 'gps',
 'datum': '83',
 'latitude': '39.058511',
 'longitude': '-121.928466',
 'oil_gas_production': [['1/1/2018', 0, 1],
  ['12/1/2017', 0, 1],
  ['11/1/2017', 0, 0],
  ['10/1/2017', 0, 126],
  ['9/1/2017', 0, 192],
  ['8/1/2017', 0, 250],
  ['7/1/2017', 0, 196],
  ['6/1/2017', 0, 183],
  ['5/1/2017', 0, 210],
  ['4/1/2017', 0, 225],
  ['3/1/2017', 0, 272],
  ['2/1/2017', 0, 205],
  ['1/1/2017', 0, 228],
  ...]
}
                        
```

## Data Processing

* All missing values are replaced with `None`

## Concurrent HTTP requests


The script currently utilizes 4 threads to make concurrent HTTP requests, which already significantly reduces the parse time compared to a single threaded scraper. We can make the script even faster. If we had multiple computer servers, we could run `scraper_threaded.py` on the different computer servers and combine the results at the end. Suppose we had 100,000 apis and 4 servers. We could run `python scraper_threaded.py 0 25000`, `python scraper_threaded.py 25000 50000`, `python scraper_threaded.py 50000 75000`, and `python scraper_threaded.py 75000 100000` on the different servers and combine the jsons at the end, which would reduce the run time by a factor of 4. We could easily dockerize the code to make it easy to run on the different servers.